{"componentChunkName":"component---src-templates-markdown-template-js","path":"/projects/smog/experiments/","result":{"data":{"markdownRemark":{"html":"<p><a href=\"/projects/smog\"><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 18%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAFxGAABcRgEUlENBAAABMElEQVQY0xXMTUoCAQCAUW8RBIFgCxf9GJTJqJOaODr5O+M4amNpOqUpGpWJIKEYVFRkmxJaRuCmTauWdYBW7TpA1/iqd4BnqTcHmNoedTlHV9W5VCKMDZm3vSW+zqz8vCzy/SzxMRCYmAuMjGV6+VUO0j46eYFqVqKkRykVkpSLSSzmVotCzGBfSnEcjtBXA1ypQSabTt4bc3x27bweCIzLIvfGPDd5BxshD771ADXNQzMtoEX8aLKXdNSFpdfq0KkesquXqGpZjnJxBhmJu3yYJ1PkccfBdVHkwpAYKTbOlVlsgsC0O4iueGln3GSifpSYj3hc/A9PaP+FzXKDprlPu1bhtJJhWFbpG25OCivcbid4UNcYylZqCTszgsiUK4iSCjDQnZgpD1k9hJoO8AtSPKd1CHFjfgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"SMOG\"\n        title=\"SMOG\"\n        src=\"/static/dc670b887bda920a35e4c875c3e3f279/5a190/smog.png\"\n        srcset=\"/static/dc670b887bda920a35e4c875c3e3f279/772e8/smog.png 200w,\n/static/dc670b887bda920a35e4c875c3e3f279/e17e5/smog.png 400w,\n/static/dc670b887bda920a35e4c875c3e3f279/5a190/smog.png 800w,\n/static/dc670b887bda920a35e4c875c3e3f279/587b0/smog.png 970w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></a></p>\n<p>The question of whether it is possible to characterise grammatical\nknowledge in probabilistic terms is central to determining the\nrelationship of linguistic representation to other cognitive domains. We\npresent a statistical model of grammaticality which maps the\nprobabilities of a statistical model for sentences in parts of the BNC\ninto grammaticality scores, using various functions of the parameters of\nthe model. Experiments with a classifier on test sets containing\ndifferent levels of syntactic infelicity have achieved encouraging\nlevels of accuracy. These results suggest that it may be possible to\ncharacterise grammaticality judgements in probabilistic terms using an\nenriched language model (for more details, see\n<a href=\"/6ce56d8b54e660d81232e17981ac1041/cgl_cogsci13.pdf\">here</a>).</p>\n<p>In a different set of experiments, we use a set of enriched n-gram\nmodels to track grammaticality judgements for various kinds of passive\nsentences in English. These experiments indicate that our n-gram models\nachieve high accuracy in identifying ill-formed passives in which\nill-formedness depends on local relations within the n-gram frame, but\nthey are far less successful in detecting non-local relations which\nproduce unacceprability in other types of passive construction. We take\nthese results to indicate some of the strengths and limitations of word\nand lexical class n-gram models as candidate representations of\nspeakers' grammatical knowledge (for more details, see\n<a href=\"/6940232ddcaffe1339c881479ba2ca09/cgl_cmcl13.pdf\">here</a>).</p>\n<p><a href=\"graph1.png\">A sample graph of the performance of our n-gram models on the passive\ndata</a>.</p>\n<p>In our Cognitive Science 2014 paper <a href=\"/dbb2f5bc113d5eca25fad960c1cba83b/lcl_cogsci14.pdf\">Measuring Gradience in Speakers'\nGrammaticality\nJudgements</a>\nwe present evidence that grammaticality is a gradient rather than a\nbinary property. Details of the experiments and the results are given in\nthe paper.</p>\n<p>In recent work we use unsupervised language models trained on corpora\nfrom a variety of domains and languages to predict speakers'\ngrammatical acceptability judgements. We apply normalising measures to\nthe probability distributions that these models generate in order to\nfilter out the effects of sentence length and word frequency. These\nmeasures give us predicted grammatical acceptability scores. We evaluate\nthese models against the mean acceptability judgements for crowd sourced\nannotated test sets in which grammatical infelicities have been\nintroduced. We also test the models against sets of crowd sourced\nannotated linguists' examples.</p>\n<p>This work is described in our ACL 2015 paper <a href=\"/a7d2a36a5781324d4083b30ef574c1c2/lcl_acl15.pdf\">Unsupervised Prediction of Acceptability Judgements</a>, and in a recent conference paper <a href=\"/0ccd6e3b2a183b81806d692a27c1cd49/lcl_iscol2015.pdf\">Predicting Acceptability Judgements with Unsupervised Language Models</a>.</p>\n<h2>Datasets</h2>\n<p>The annotated data sets for our experiments are available below, and the\nopen source toolkit for running our unsupervised models can be accessed\nfrom our software page. Datasets with human-annotated acceptability\nratings (all files are tab-delimited csv files):</p>\n<ul>\n<li><a href=\"/c13a5a8341fe5989bcbf3006920d91a7/adger.csv\">Adger</a></li>\n<li><a href=\"/f9e3baa080636f3929ad1de39a047f23/adger_filtered.csv\">Adger Filtered</a></li>\n<li><a href=\"/914a288ca1e127a7f1547412d9a7e056/bnc.csv\">BNC</a></li>\n<li><a href=\"/b56bd338580fbb90b14e276266633abc/enwiki.csv\">English Wikipedia</a></li>\n<li><a href=\"/c95468dbbedc548fec5c68c2e6a10c54/eswiki.csv\">Spanish Wikipedia</a></li>\n<li><a href=\"/822df52f3776e355410ecaade763d5b0/dewiki.csv\">German Wikipedia</a></li>\n<li><a href=\"/82f95eb9cc6b15c3f66ff27c4f7137fe/ruwiki.csv\">Russian Wikipedia</a></li>\n</ul>\n<p><a href=\"https://esrc.ukri.org/\"><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 220px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 57.00000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAACYElEQVQoz1WTW1NSURTH92nqoYesrCQVQkBRiJtxB7nKHZEjoIIicQkJb+ElmsacKaep9177TvsT7ZnlWnumwR7W2fucs/dv/9d/rc1eGjbZrLmi0PhiocTN7/YBQzzTFWF2aQtMzgbIubkCVl9Ljk/mcjC/UhW2QBtwH9e/3WE667ZkMHqYXA2FTUXZU22BN/s/IbkxEgSL5E6hNfgFuBhcaz24vv0LzlAXlt1NqDZvxNLqPsyYylxv22FaS20CNDrqysPpJEMlfGv/G6TKFwK/w6JrD/Z6tzC/XIUVzwGcjf/AauQDvMbDittfhD3QIcXcFe4xR7AzAeJGBU9lGpPKA+tD8Cc/Cqu3JdXQO432YAe88YEMmnvih8KXGMArY5kv2HYZWjMBOkJdBdNk6AtPbV7AeulcxPJnEEofQ6E2lmkniiPIbl3JeaV5A/XOD2H1vQfynRiYxQSIpiqPNRn2/E1RAtF8gSohXb6UPq5lTyCSPYVg6kgqJLg/ORSkfmo+z9kjP0MvJ0CL50CZW67KKuPJqOxIoGpIls4hVjiTAHe0D+7YoQRm1EuIFz+JKGbxTyFaNgE6w10Fg2kWVb7b/k4QoTaugdKmIoUzJxKyWf8K7eFvCaXCUcF0lhqJYImN0T2F3paCRUDZKs9VPiPgWOSrY4hiuhn0LaNewTpakUYo/afUESDIDmobKuh/VaYXXMi0KzVOXuECQRBSSJ5iW1CbEATwcEy9L20hn6WHD7xsxnjPQ0xVmdaXpIcG+y41sjA46vJW0E2hPsTGlYE9C1hECkENTjcF11GHSOAd3zM+BgG7634AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"ESRC Logo\"\n        title=\"ESRC Logo\"\n        src=\"/static/3c4a8e2d0cb055b240873fdbe7587e4a/c8042/esrc_logo.png\"\n        srcset=\"/static/3c4a8e2d0cb055b240873fdbe7587e4a/772e8/esrc_logo.png 200w,\n/static/3c4a8e2d0cb055b240873fdbe7587e4a/c8042/esrc_logo.png 220w\"\n        sizes=\"(max-width: 220px) 100vw, 220px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></a></p>","frontmatter":{"title":"Experiments and Datasets","date":null},"fields":{"slug":"/projects/smog/experiments/"}}},"pageContext":{"slug":"/projects/smog/experiments/"}},"staticQueryHashes":["3875542623"]}