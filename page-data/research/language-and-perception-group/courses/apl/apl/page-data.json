{"componentChunkName":"component---src-templates-markdown-template-js","path":"/research/language-and-perception-group/courses/apl/apl/","result":{"data":{"markdownRemark":{"html":"<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/fc9c71d76312d207d482a2c94d3edb8e/d2602/IMG_5943.jpg\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 75%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAUBAgT/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAv/aAAwDAQACEAMQAAAB0VUzUMxSH//EABsQAAEEAwAAAAAAAAAAAAAAAAABAgMREBNB/9oACAEBAAEFAqjUdpQuM5j/xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAwEBPwGI/8QAFREBAQAAAAAAAAAAAAAAAAAAABL/2gAIAQIBAT8Btb//xAAaEAACAgMAAAAAAAAAAAAAAAAAEAEyAiFh/9oACAEBAAY/AtZwWLLi/8QAGhAAAwADAQAAAAAAAAAAAAAAAAERITFxgf/aAAgBAQABPyGSiZu74qJi19MsZlFP/9oADAMBAAIAAwAAABBH3//EABURAQEAAAAAAAAAAAAAAAAAAAAR/9oACAEDAQE/EJS//8QAFxEBAQEBAAAAAAAAAAAAAAAAAQARQf/aAAgBAgEBPxAT21f/xAAbEAEBAQEBAAMAAAAAAAAAAAABEQAxUUFxgf/aAAgBAQABPxAuH6Xmgrpzu+8QZ34DcIkX9ymylRe6HlT3f//Z'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"IMG 5943\" title=\"IMG 5943\" src=\"/static/fc9c71d76312d207d482a2c94d3edb8e/4b190/IMG_5943.jpg\" srcset=\"/static/fc9c71d76312d207d482a2c94d3edb8e/e07e9/IMG_5943.jpg 200w,\n/static/fc9c71d76312d207d482a2c94d3edb8e/066f9/IMG_5943.jpg 400w,\n/static/fc9c71d76312d207d482a2c94d3edb8e/4b190/IMG_5943.jpg 800w,\n/static/fc9c71d76312d207d482a2c94d3edb8e/e5166/IMG_5943.jpg 1200w,\n/static/fc9c71d76312d207d482a2c94d3edb8e/b17f8/IMG_5943.jpg 1600w,\n/static/fc9c71d76312d207d482a2c94d3edb8e/d2602/IMG_5943.jpg 4032w\" sizes=\"(max-width: 800px) 100vw, 800px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n  </a>\n    </span>\n<p>APL HT18 and onwards, Language, Action, and Perception, 7.5 HEC, Språk, handling och perception, 7,5hp, part of <a href=\"https://medarbetarportalen.gu.se/digitalAssets/1656/1656982_asp-fd-datalingvistik-2016-eng.pdf\">Doctoral Degree in Computational\nLinguistics</a>.</p>\n<p>This is PhD course that explores computational modelling of language and vision in particular in relation to situated dialogue agents and image classification. There is a parallel course at the masters level which this course may partially overlap with: LT2308 ESLP: Embodied and Situated Language Processing or LT2318: Artificial Intelligence: Cognitive Systems.</p>\n<p>The course gives a survey of theory and practical computational implementations of how natural language interacts with the physical world through action and perception. We will look at topics such as semantic theories and computational approaches to modelling natural language, action and perception (grounding), situated dialogue systems, integrated robotic systems, grounding of language in action and perception, generation and interpretation of scene descriptions from images and videos, spatial cognition, and others.</p>\n<p>As the course studies how humans structure and interact with the physical world and express it in language, it bridges into the domains of cognitive science, computer vision, robotics and therefore more broadly belongs to the field of cognitive artificial intelligence. Typical applications of computational models of language, action, and perception are image search and retrieval on the web, navigation systems that provide more natural, human-like instructions, and personal robots and situated conversational agents that interact with us in our home environment through language.</p>\n<p>The learning outcomes of the course are based on covering 3 topics: (i) the relation between language and perception in human interaction, (ii) how language and perception is modelled with formal and computational models and methods and how these are integrated with different applications, and (iii) how research in the field is communicated scientifically.</p>\n<p>Course prerequisites:</p>\n<ul>\n<li>General admission requirements for a doctoral degree in Computational Linguistics or equivalent.</li>\n</ul>\n<p>In order to follow the course, the participants should at least have experience with one or several of the following fields at masters level:</p>\n<ul>\n<li>Formal semantics and pragmatics</li>\n<li>Natural language processing</li>\n<li>Computational semantics</li>\n<li>Machine learning</li>\n<li>Robotics</li>\n<li>or equivalent skills and knowledge.</li>\n</ul>\n<p>Course syllabus</p>\n<ul>\n<li><a href=\"/252d2215499653c932c9eaafd8ad4487/Language,%20Action%20and%20Perception.pdf\">In Swedish</a></li>\n</ul>\n<h2>Requirements</h2>\n<p>Please read <a href=\"/ee40cbbcc8ef5b736efb415fdcc56c44/requirements.md\">this document</a> and talk to Simon.</p>\n<h2>Lecturers</h2>\n<ul>\n<li><a href=\"https://www.gu.se/en/about/find-staff/simondobnik\">Simon Dobnik</a> (course organiser), office hours: by appointment</li>\n</ul>\n<h2>Course literature</h2>\n<p>For a list of suggested readings please see <a href=\"https://gu-clasp.github.io/language-and-perception/meetings/\">here</a>. Individual readings will be suggested for each meeting.</p>\n<h2>Schedule and course materials</h2>\n<ul>\n<li>\n<p><strong>Contextual referrring expressions</strong> </p>\n<ul>\n<li>2020-06-12, Zoom</li>\n<li>Pezzelle, S., &#x26; Fernández, R. (2019). <a href=\"https://arxiv.org/pdf/1908.10285.pdf\">Is the Red Square Big? MALeViC: Modeling Adjectives Leveraging Visual Contexts.</a> arXiv preprint arXiv:1908.10285. (recommended by Staffan) 2020-06-12 </li>\n<li>Staffan (presenter), Tewodros, Maryam, Mehdi, Simon, and Robin</li>\n</ul>\n</li>\n<li>\n<p><strong>Visual question answering and background knowledge</strong></p>\n<ul>\n<li>2020-05-29, Zoom</li>\n<li>Talk: Míriam Sánchez-Alcón: <a href=\"https://gubox.app.box.com/s/djn8w0k2qlmkgbdsr8yk0dsz22r1fjsj\">The significance of applying attention to Visual Question Answering</a></li>\n<li>Wu, J., &#x26; Mooney, R. J. (2018). <a href=\"http://arxiv.org/abs/1809.02805\">Faithful Multimodal Explanation for Visual Question Answering</a> [cs.CL], 2020. (recommended by Simon) 2020-05-29</li>\n<li>Nikolai (presenter), Miriam (presenter), Tewodros, Robin, Staffan, Simon</li>\n</ul>\n</li>\n<li>\n<p><strong>Word complexity and concreteness, requirements for social and embodied NLP</strong></p>\n<ul>\n<li>2020-04-30, Zoom</li>\n<li>Talk: David Alfter: <a href=\"https://gubox.box.com/shared/static/nuyn4p02bcj8pok1lmd9huf54wfmt8an.pdf\">Visual features in textual complexity classification: a case study on pictograms</a></li>\n<li>Y. Bisk, A. Holtzman, J. Thomason, J. Andreas, Y. Bengio, J. Chai, M. Lapata, A. Lazaridou, J. May, A. Nisnevich, N. Pinto, and J. Turian. <a href=\"https://arxiv.org/abs/2004.10151\">Experience grounds language.</a> arXiv, arXiv:2004.10151 [cs.CL], 2020. 2020-04-30</li>\n<li>Robin, Staffan, Mehdi, Nikolai, Bill, Vlad, Tewodros, Maryam, David, Elena, Simon</li>\n</ul>\n</li>\n<li>\n<p><strong>Generating image descriptions, natural language generation</strong></p>\n<ul>\n<li>2020-04-17, Zoom</li>\n<li>J. Krause, J. Johnson, R. Krishna, and L. Fei-Fei. <a href=\"https://arxiv.org/pdf/1611.06607.pdf\">A hierarchical approach for generating descriptive image paragraphs.</a> In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3337–3345, July 21–26 2017.</li>\n<li>Nikolai (presenter), Mehdi, Robin, Vlad, Bill, Aram, Maryam, and Simon</li>\n</ul>\n</li>\n<li>\n<p><strong>Generating image descriptions and pragmatics</strong></p>\n<ul>\n<li>2020-03-20, Zoom</li>\n<li>Cohn-Gordon, R., Goodman, N., &#x26; Potts, C. (2018). <a href=\"http://arxiv.org/abs/1804.05417\">Pragmatically Informative Image Captioning with Character-Level Inference.</a></li>\n<li>Nikolai (presenter), Mehdi, Robin, Vlad, Bill, Tewodros and Simon (check)</li>\n</ul>\n</li>\n<li>\n<p><strong>Spatial representations, representation learning, interpretability</strong></p>\n<ul>\n<li>2019-02-08 10-12 Dicksonsgatan 4</li>\n<li>G. Collell, L. V. Gool, and M. Moens. <a href=\"http://arxiv.org/abs/1711.06821\">Acquiring common sense spatial knowledge through implicit spatial templates.</a> arXiv, arXiv:1711.06821 [cs.AI]:1–8, 2017.</li>\n<li>Mehdi (presenter), Felix, Vlad, Robin, Staffan, Simon</li>\n</ul>\n</li>\n<li>\n<p><strong>Language and action</strong></p>\n<ul>\n<li>2019-03-08 10-12 Dicksonsgatan 4</li>\n<li>Forestier S, Oudeyer P-Y. (2017) <a href=\"http://sforestier.com/node/32\">A Unified Model of Speech and Tool Use Early Development.</a> Proceedings of the 39th Annual Meeting of the Cognitive Science Society. </li>\n<li>Sylvie (presenter), Felix, Mehdi, Bill, Robin, Stergios, Simon</li>\n</ul>\n</li>\n</ul>\n<p>You can find  an earlier version of this webpage <a href=\"/74682c28ab17c7a660752b885fe6b6ed/archived.zip\">here</a>.</p>","frontmatter":{"title":"Language, Action, and Perception (APL)","date":null},"fields":{"slug":"/research/language-and-perception-group/courses/apl/apl/"}}},"pageContext":{"slug":"/research/language-and-perception-group/courses/apl/apl/"}},"staticQueryHashes":["3875542623"]}