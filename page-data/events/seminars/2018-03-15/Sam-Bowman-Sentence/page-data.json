{"componentChunkName":"component---src-templates-seminar-template-js","path":"/events/seminars/2018-03-15/Sam-Bowman-Sentence/","result":{"data":{"markdownRemark":{"html":"<p>Artificial neural networks now represent the state of the art in most large-scale applied language understanding tasks. This talk presents a few methods and results, organized around the task of recognizing textual entailment, which measure the degree to which these models can or do learn something resembling compositional semantics. I discuss experiments on artificial data and on a hand-built million-example corpus of natural data (SNLI/MultiNLI), and report encouraging results.</p>\n<p>ReferencesBowman, Samuel R., Christopher Potts, and Christopher D. Manning. \"Recursive neural networks can learn logical semantics.\" arXiv preprint arXiv:1406.1827 (2014).</p>","frontmatter":{"title":"Sentence Understanding with Neural Networks and Natural Language Inference","lecturer":"Sam Bowman","duration":"2 hours","date":"15 Mar, 2018","venue":"Gothenburg","slides":{"publicURL":"/static/832f3932063fe0cdfc9dc09468cef619/1684992_go--teborg-ii.pdf"}},"fields":{"slug":"/events/seminars/2018-03-15/Sam-Bowman-Sentence/"}}},"pageContext":{"slug":"/events/seminars/2018-03-15/Sam-Bowman-Sentence/"}},"staticQueryHashes":["3875542623"]}