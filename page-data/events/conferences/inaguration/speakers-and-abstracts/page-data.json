{"componentChunkName":"component---src-templates-conference-template-js","path":"/events/conferences/inaguration/speakers-and-abstracts/","result":{"data":{"markdownRemark":{"html":"<ul>\n<li><a href=\"./programme\">Programme</a></li>\n<li><a href=\"./registration\">Registration</a></li>\n<li><a href=\"./speakers-and-abstracts\">Speakers and Abstracts</a></li>\n<li><a href=\"./videos\">Videos</a></li>\n</ul>\n<h1>Speakers and abstracts</h1>\n<h2>Speakers</h2>\n<ul>\n<li>Lars Borin, University of Gothenburg</li>\n<li>Ellen Breitholtz, University of Gothenburg</li>\n<li>Stegios Chatzikyriakidis, University of Gothenburg, CLASP member</li>\n<li>Alexander Clark, King's College London, CLASP Scientific Advisory\nCommittee</li>\n<li>Simon Dobnik, University of Gothenburg, CLASP member</li>\n<li>Devdatt Dubhashi, Chalmers University of Technology</li>\n<li>Katrin Erk, University of Texas at Austin, CLASP Scientific Advisory\nCommittee</li>\n<li>Grants and Innovation Office representative, University of\nGothenburg</li>\n<li>Margareta Hallberg, Dean, Faculty of Arts</li>\n<li>Chris Howes, University of Gothenburg</li>\n<li>Shalom Lappin, University of Gothenburg and King's College London,\nCLASP Director</li>\n<li>Staffan Larsson, University of Gothenburg, CLASP member</li>\n<li>Joakim Nivre, Uppsala University, CLASP Scientific Advisory\nCommittee</li>\n<li>Aarne Ranta, Chalmers University of Technology</li>\n<li>Charalambos Themistocleous, University of Gothenburg and Princeton\nUniversity, CLASP member</li>\n<li>Christina Thomsen Thörnqvist, Prefekt, FLOV</li>\n</ul>\n<h2>Abstracts</h2>\n<h3>Lars Borin, University of Gothenburg: Multi-word expressions - eels in sheep's clothing?</h3>\n<p>Multi-word expressions (MWEs) have attracted much attention in NLP over\nthe last decade or so, and in general linguistics, the interest in\nphraseology - which includes the linguistic study of MWEs - goes back\nmuch further. However, the broad comparative approach characteristic of\nresearch in linguistic typology seems not to have played any role in any\nof this work so far. On the contrary, comparative studies of MWEs in NLP\n(or phraseology in linguistics) have generally been contrastive rather\nthan typological in scope, i.e., they deal with (a convenience sample\nof) a few languages, rather than with a systematic typological sample\nrepresentative of the world's linguistic diversity, with the result that\nno unitary cross-linguistically valid notion of MWE can be found in the\nliterature. Approaching MWEs from a broad cross-linguistic perspective\nraises a number of intriguing theoretical and methodological questions,\nfor linguistics and NLP alike. In fact, closer connections between NLP\nwork on MWEs and linguistic research on lexical and semantic typology\ncould have an important role to play for developing new research\ndirections in both fields.</p>\n<h3>Stergios Chatzikyriakidis, University of Gothenburg: Modern Type Theoretical Semantics, Inference and Probability Theory</h3>\n<p>In this talk I report on-going work on the use of Modern Type Theories\n(MTTs), i.e. type theories within the tradition of Martin Löf (1974,\n1981), in the study of linguistic semantics. In particular, I exemplify\nthe use of Luo's type theory with coercive sub-typing and show its\napplicability for a wide range of semantic phenomena, including\nadjectival/adverbial modification, co-predication and belief\nintensionality among others. I will then argue that the proof-theoretic\nnature of MTTs has the further advantage that these can be further\nimplemented into reasoning engines in order to perform reasoning tasks.\nIn particular, this proof-theoretic aspect has been the main reason that\na number of proof-assistants implement variants of MTTs. One such\nproof-assistant, Coq, will be used as a way to show the applicability of\nMTTs in dealing with Natural Language Inference (NLI). Lastly, I will\ndiscuss the issue of introducing probabilities into type theory. In\nparticular, I want to focus on the problems that such endeavour might\nface and discuss possibilities on how such an extension of MTTs can be\nat least initiated.</p>\n<h3>Alex Clark, King's College London: Learning Syntactic Structure: Weak Learning, Strong Learning and Canonical Grammars</h3>\n<p>The origin of syntactic structure has been a difficult problem for\ntheoretical linguistics for many decades. One standard assumption is\nthat it must be bootstrapped in some way from some external source of\ninformation, typically some hierarchically structured semantic\nrepresentation. We will explore a radical alternative to this view: that\ndistributional patterns in the raw data may suffice. Under some\ncircumstances formal languages will have a unique canonical grammar\nwhich can form the basis for a learning algorithm which recovers a\nreasonable notion of syntactic structure: this approach relies on\nidentifying irreducible elements of an algebraic structure - the\nsyntactic concept lattice - canonically associated with every formal\nlanguage. </p>\n<h3>Simon Dobnik, University of Gothenburg: On Interfacing Language, Spatial Perception, Dialogue, and Cognition</h3>\n<p>I will give an overview of our work on building computational models of\nmeaning of spatial descriptions in dialogue interaction such as \"the\nchair is to the left of the table\" or \"turn right at the next\ncrossroad\" which include both linguistic and perceptual\nrepresentations, for example those used in computer vision and robotics.\nAs such models interface perceptional and conceptual domains they\ninvariably require an application of statistical models and machine\nlearning. Physical sciences have developed ways in which space can be\ndescribed with high degree of accuracy, for example by measuring\ndistances and angles. Such measures can be represented on a continuous\nscale of real numbers. However, humans refer to space quite differently:\nthey use reference to discrete units such as points, regions and volumes\nand they also take into account what they know about the world and the\nobjects, for example the dynamic kinematic routines between them.\nSpatial descriptions are also notoriously underspecified and vague and\nthey have to be interpreted against appropriate perceptual and discourse\ncontexts. In my ongoing work with Robin Cooper, Shalom Lappin and\nStaffan Larsson on Type Theory with Records (TTR) I have tried to give\nthis practical experience theoretical foundations by exploring how such\nmodels relate to linguistic theory, in particular to formal semantics,\nand use the models as a test-bed for theory development.</p>\n<h3>Devdatt Dubhashi, Chalmers University of Technology: Distributed representations in NLP</h3>\n<p>Distributed representations such as Google's word2vec and Stanford's\nGloVe which have emerged out of the Deep Learning research community,\nhave been shown to be capture deep semantic information and thus\nconstitute powerful and highly scalable data driven frameworks for NLP.\nWe show examples from work of our research group of how they can be used\nfor word sense induction and automatic document summarisation and how\nthey can be extended to capture time dynamics of language change. </p>\n<h3>Katrin Erk, University of Texas at Austin: Semantics as a Heterogeneous Mess, and How to Reason Over It</h3>\n<p>Many phenomena in lexical semantics seem to involve gradedness. Synonymy\nis a case at hand: Instead of absolute synonymy, we find near-synonymy\nof words that are often substitutable but still differ in nuances of\nmeaning. Polysemy also seems to come in degrees, with different uses of\na word differing in their perceived similarity. We use distributional\nmodels to describe degrees of similarity of word instances, and combine\nthem with logical form representations of sentence meaning. In this\ntalk, we show how to use Markov Logic Networks (MLNs) to perform\nprobabilistic inference over logical form with weighted distributional\ninference rules for the task of Recognising Textual Entailment (RTE). We\nalso speculate how a human agent could make use of distributional\ninformation and integrate it with everything else they know through a\nprobabilistic framework. We argue that if semantics is a heterogeneous\nmess (which seems likely), it is important to find the right\nprobabilistic framework for reasoning over it.</p>\n<h3>Chris Howes, University of Gothenburg: Incremental Reasoning in Dialogue (IncReD), work with Ellen Breitholtz and Robin Cooper</h3>\n<p>In this talk, we will outline our grant proposal 'IncReD' (Incremental\nReasoning in Dialogue). This project aims to extend insights on\nincrementality in language processing beyond the utterance level.\nReasoning - which often plays the role of providing coherence and\nstructure in larger chunks of language - is also incremental in the\nsense that we tend to form hypotheses regarding the arguments of our\nconversational partners before these arguments are fully explicit. In\nthis sense incrementality in reasoning is analogous to syntactic and\nsemantic incrementality. We aim to combine insights from a variety of\nfields (e.g. Artificial Intelligence, Formal Linguistics,\nPsycholinguistics, Philosophy) and use corpus methods, state of the art\nexperimental techniques (e.g. the Dialogue Experimental Toolkit (DiET))\nand formal models from syntactic, semantic and pragmatic domains (e.g.\nDynamic Syntax (DS) and Type Theory with Records (TTR)) to develop a\nmodel of dialogue that accounts for a range of dialogue phenomena\nincluding reasoning. Specifically we intend to investigate: (1) What\ntypes of reasoning do people use in dialogue? What resources does this\nreasoning rely on and how are these resources accessed incrementally?\n(2) What happens in a dialogue (linguistically and interactionally) when\nthere is a mismatch in the resources for reasoning between participants?\nWhat factors influence the arguments a person uses when conflicting\nresources are available? (3) How can this incremental human reasoning\nability be formally modelled? </p>\n<h3>Shalom Lappin, King's College London and University of Gothenburg: A Probabilistic View of Grammaticality Joint work with Jey Han Lau and Alexander Clark, King's College London</h3>\n<p>The question of whether grammatical competence should be represented by\na formal grammar that provides a binary membership condition for the set\nof well-formed sentences (and their associated structures) in a\nlanguage, or as a probabilistic system for determining relative values\nof grammatical acceptability has been a central issue in computational\nlinguistics and cognitive science over the past two decades. In this\ntalk I will present experimental evidence that speakers' judgements of\ngrammatical acceptability are intrinsically gradient. I will show that\nunsupervised language models, augmented with grammatical scoring\nfunctions, can predict these judgements with an encouraging degree of\naccuracy over distinct domains and different languages. These results\nprovide motivation for the view that grammatical competence is a\nprobabilistic system. They also raise interesting questions about the\nnature of the language acquisition process. One of the main concerns of\nthe talk will be to clarify the relationship between grammatical\nacceptability and probability. This work was done within the framework\nof my Economic and Social Research Council of the UK project Statistical\nModels of Grammaticality (SMOG).</p>\n<h3>Staffan Larsson, University of Gothenburg: Vagueness and Learning in Probabilistic TTR, work with Raquel Fernandez</h3>\n<p>We present a formal account of the meaning of vague scalar adjectives\nsuch as \"tall\" formulated in probabilistic Type Theory with Records.\nOur approach makes precise how perceptual information can be integrated\ninto the meaning representation of these predicates; how an agent\nevaluates whether an entity counts as tall; and how the proposed\nsemantics can be learned and dynamically updated through experience.</p>\n<h3>Joakim Nivre, Uppsala University: Towards a Universal Grammar for Natural Language Processing</h3>\n<p>Universal Dependencies is a recent initiative to develop\ncross-linguistically consistent tree-bank annotation for many languages,\nwith the goal of facilitating multilingual parser development,\ncross-lingual learning, and parsing research from a language typology\nperspective. In this talk, I outline the motivation behind the\ninitiative and explain how the basic design principles follow from these\nrequirements. I then discuss the different components of the annotation\nstandard, including principles for word segmentation, morphological\nannotation, and syntactic annotation. I conclude with some thoughts on\nthe challenges that lie ahead.</p>\n<h3>Aarne Ranta, Chalmers University of Technology: What are grammars good for?</h3>\n<p>In formal language theory, a language is a set of strings and a grammar\nis an inductive definition of this set. It generates all and only the\nvalid strings of the language. In linguistics, generative grammars have\nsimilarly aimed at defining all and only the valid sentences of natural\nlanguages - in other words, the notion of grammaticality. One objection\nto this idea is that grammaticality in natural language is not a binary\nnotion but a gradient one. Does this imply that grammars are useless and\nshould be replaced by probabilistic models of language?</p>\n<p>In our view, not. We should just abandon the idea that the purpose of\ngrammars is to define grammaticality. Grammars should rather be seen as\nways of structuring data. Even statistical language models need\ngrammars, often just very simple ones, For instance, a grammar might\nhave the sequences of strings as its only structure. But we want to show\nhow richer grammatical structures - often together with statistics - are\na useful model that can for instance compensate for sparse data. We will\nalso discuss the wide-spread beliefs that hand-written grammars cannot\nbe robust and that they require too much work to be useful in practice.</p>\n<h3>Charalambos Themistocleous, University of Gothenburg and Princeton University: Learning Linguistic Categories from Acoustic Structure: Towards a Speech Time Frequency Model</h3>\n<p>[Linguistic information is hard-coded in speech signal. By analysing\nspecific acoustic properties, such as vowel formants and fundamental\nfrequency, acoustic models of speech production aim to elicit this\ninformation. I summarise evidence from my research on speech processing\nand argue that these acoustic models provide only incomplete spectral\ndescription and under-represent interactions between acoustic\nproperties. Consequently, they do not do justice to the complex\nlinguistic information encoded in speech. I then propose a model for\nspeech processing based on parameterised resonant signal elements and an\nalgorithm that analyses vowel samples based on the proposed model. The\nalgorithm provides a rich description of any given segmented vowel\nsample by using a large number of resonant elements with parameters that\nare chosen to accurately capture the time-frequency structure of the\nvowel. The parameters are then used to calculate probabilities. An\napplication of the model successfully classifies vowels, stress, and\nspeech variety. This model is an improvement over methods that only use\na small number of formants to describe vowels, has the potential to be\nused in automatic speech recognition, and is promising for use in\napplications of forensic linguistics, and speech pathology. Finally, I\ndiscuss an ongoing work that aims to extend the model for the analysis of\nprosody.</p>","frontmatter":{"title":"Language and Probability: The CLASP Inauguration Workshop","duration":"1 day","date":"27 Aug, 2015","venue":"Gothia Towers (Gothenburg)","externalSite":null,"proceedings":null},"fields":{"slug":"/events/conferences/inaguration/speakers-and-abstracts/"}}},"pageContext":{"slug":"/events/conferences/inaguration/speakers-and-abstracts/"}},"staticQueryHashes":["3875542623"]}