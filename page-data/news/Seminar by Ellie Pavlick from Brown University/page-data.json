{"componentChunkName":"component---src-templates-news-template-js","path":"/news/Seminar by Ellie Pavlick from Brown University/","result":{"data":{"markdownRemark":{"html":"<p>Cordially welcome to a seminar by Ellie Pavlick from Brown University on Wednesday 18th of November, at 15:00 online via Zoom. </p>\n<p>Title: \"You can lead a horse to water...: Representing vs. Using Features in Neural NLP\"</p>\n<p>Abstract: A wave of recent work has sought to understand how pretrained language models work. Such analyses have resulted in two seemingly contradictory sets of results. On one hand, work based on \"probing classifiers\" generally suggests that SOTA language models contain rich information about linguistic structure (e.g., parts of speech, syntax, semantic roles). On the other hand, work which measures performance on linguistic \"challenge sets\" shows that models consistently fail to use this information when making predictions. In this talk, I will present a series of results that attempt to bridge this gap. Our recent experiments suggest that the disconnect is not due to catastrophic forgetting nor is it (entirely) explained by insufficient training data. Rather, it is best explained in terms of how \"accessible\" features are to the model following pretraining, where \"accessibility\" can be quantified using an information-theoretic interpretation of probing classifiers.</p>\n<p>Time: 15.00-16.45</p>\n<p>Location: via Zoom, <a href=\"https://gu-se.zoom.us/j/63017385241?pwd=NEdYMkRCUnRUTVQ4UUtuUTM3aDdEUT09\">https://gu-se.zoom.us/j/63017385241?pwd=NEdYMkRCUnRUTVQ4UUtuUTM3aDdEUT09</a></p>","frontmatter":{"title":"Seminar by Ellie Pavlick from Brown University","date":"November 09, 2020","bannerImage":{"publicURL":"/static/c9af1a4d3ace5757c1e05c398aa48acd/meeting-311355_1280.png"}},"fields":{"slug":"/news/Seminar by Ellie Pavlick from Brown University/"}}},"pageContext":{"slug":"/news/Seminar by Ellie Pavlick from Brown University/"}},"staticQueryHashes":["3875542623"]}