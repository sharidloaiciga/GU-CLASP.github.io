{"componentChunkName":"component---src-templates-news-template-js","path":"/news/Using and comparing contextual embeddings across languages/","result":{"data":{"markdownRemark":{"html":"<h3>Matthew Purver \"Transferable skills: using and comparing contextual embeddings across languages\"</h3>\n<p>This talk will present ongoing work on the EMBEDDIA H2020 project, in which the overall aim is to develop language tools which can be quickly and easily deployed across languages without substantial data resources. We will look at an example task, automated comment moderation for news articles, and see that while it can be approached as a standard supervised classification task, good accuracy is hard to achieve with limited resources. We will introduce the concept of cross-lingual embeddings, and explain how they may offer a solution to this problem via transfer learning. We will describe a new dataset, CoSimLex, developed to help evaluate and compare word embedding models, and particularly their representation of words in context. We then see that standard models such as multilingual BERT can be outperformed by more language-specific models, both on this intrinsic evaluation task and, via cross-lingual transfer, on the comment moderation task.</p>\n<h4>Lecturer:</h4>\n<p>Matthew Purver</p>\n<h4>Date:</h4>\n<p>6/10/2020</p>\n<h4>Time:</h4>\n<p>1:15 PM - 3:00 PM</p>\n<h5>Categories:</h5>\n<p>Linguistics</p>\n<h5>Organizer:</h5>\n<p>CLASP</p>\n<h5>Location:</h5>\n<p>Zoom:</p>\n<h4>Contact person:</h4>\n<p>Stergios Chatzikyriakidis</p>","frontmatter":{"title":"Using and comparing contextual embeddings across languages","date":"June 10, 2020","bannerImage":{"publicURL":"/static/c9af1a4d3ace5757c1e05c398aa48acd/meeting-311355_1280.png"}},"fields":{"slug":"/news/Using and comparing contextual embeddings across languages/"}}},"pageContext":{"slug":"/news/Using and comparing contextual embeddings across languages/"}},"staticQueryHashes":["3875542623"]}