{"componentChunkName":"component---src-templates-news-template-js","path":"/news/Seminar by Olof Mogren from RISE Research institutes of Sweden/","result":{"data":{"markdownRemark":{"html":"<p>Cordially welcome to a seminar by Olof Mogren from RISE Research institutes of Sweden on Wednesday 20th of January, at 13:15 online via Zoom. </p>\n<p>Title: \"Learned representations and what they encode\".</p>\n<p>Abstract: \"Learned continuous embeddings for language units was some of the first trembling steps of making neural networks useful for natural language processing (NLP), and promised a future with semantically rich representations for downstream solutions. NLP has now seen some of the progress that previously happened in image processing: the availability of increased computing power and the development of algorithms have allowed people to train larger models that perform better than ever. Such models also make it possible to use transfer learning for language tasks, thus leveraging large widely available datasets.</p>\n<p>In 2016, Bolukbasi, et.al., presented their paper \"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\", shedding lights on some of the gender bias that was available in trained word embeddings at the time. Datasets obviously encode the social bias that surrounds us, and models trained on that data may expose the bias in their decisions. Similarly, learned representations may encode sensitive details about individuals in the datasets; allowing the disclosure of such information through distributed models or their outputs. All of these aspects are crucial in many application areas, not the least in the processing of medical texts.</p>\n<p>Some solutions have been proposed to limit the expression of social bias in NLP systems. These include techniques such as data augmentation, representation calibration, and adversarial learning. Similar approaches may also be relevant for privacy and disentangled representations. In this talk, we'll discuss some of these issues, and go through some of the solutions that have been proposed recently to limit bias and to enhance privacy in various settings.\"</p>\n<p>Bio: Olof Mogren, PhD, is head of deep learning research at RISE. Research interests include representation learning, uncertainty quantification, and modelling the world around us, in application domains such as analysis of medical texts, image processing, and sensor modelling.</p>\n<p>Time: 13.15-15.00</p>\n<p>Location: via Zoom, <a href=\"https://gu-se.zoom.us/j/63017385241?pwd=NEdYMkRCUnRUTVQ4UUtuUTM3aDdEUT09\">https://gu-se.zoom.us/j/63017385241?pwd=NEdYMkRCUnRUTVQ4UUtuUTM3aDdEUT09</a></p>","frontmatter":{"title":"Seminar by Olof Mogren from RISE Research institutes of Sweden","date":"January 20, 2021","bannerImage":{"publicURL":"/static/c9af1a4d3ace5757c1e05c398aa48acd/meeting-311355_1280.png"}},"fields":{"slug":"/news/Seminar by Olof Mogren from RISE Research institutes of Sweden/"}}},"pageContext":{"slug":"/news/Seminar by Olof Mogren from RISE Research institutes of Sweden/"}},"staticQueryHashes":["3875542623"]}