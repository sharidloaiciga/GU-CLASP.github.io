<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta name="generator" content="Gatsby 2.32.13"/><title data-react-helmet="true"></title><link data-react-helmet="true" href="https://fonts.googleapis.com/css2?family=Roboto:wght@300&amp;display=swap" rel="stylesheet"/><link data-react-helmet="true" rel="stylesheet" type="text/css" media="all" href="/css/bootstrap.min.css"/><link data-react-helmet="true" rel="stylesheet" type="text/css" media="all" href="/css/theme.css"/><link data-react-helmet="true" rel="stylesheet" type="text/css" media="all" href="/css/style.css"/><link data-react-helmet="true" rel="stylesheet" type="text/css" media="all" href="/css/menu.css"/><meta data-react-helmet="true" name="google-site-verification" content="aLbLg9i2oE98fQuvzFSYrW6xSSAyrCk7cZxLPzDiz4s"/><script data-react-helmet="true" type="text/javascript" src="/js/jquery-3.5.1.min.js"></script><script data-react-helmet="true" type="text/javascript" src="/js/bootstrap.min.js"></script><script data-react-helmet="true" type="text/javascript" src="/js/base.js"></script><link as="script" rel="preload" href="/webpack-runtime-df4e8c76addaafc8837b.js"/><link as="script" rel="preload" href="/framework-073c72e9d2e4c2490fcd.js"/><link as="script" rel="preload" href="/app-9846c3f7f4b3f0350ee6.js"/><link as="script" rel="preload" href="/commons-5dfeb980da566c46321b.js"/><link as="script" rel="preload" href="/component---src-templates-markdown-template-js-cf19b8e806f90d375072.js"/><link as="fetch" rel="preload" href="/page-data/research/language-and-perception-group/meetings/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/3875542623.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body data-spy="scroll" data-target="#toc"><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><div class="container-fluid"><div class="row"><div class="container"><header class="jumbotron subhead mb-0" id="overview"><div class="row"><div id="clasp-header-logo" class="align-self-center col-md-3 col-12"><img src="/img/clasp.png" href="https://clasp.gu.se" alt="The Centre for Linguistic Theory and Studies in Probability"/></div><div class="align-self-center col-md-6 col-12"><div style="font-size:1.5rem">The Centre for Linguistic Theory and Studies in Probability</div></div><div class="col-md-3 col-12"><img id="gu-header-logo" src="/img/gu-logo.png" href="gu.se" alt="University of Gothenburh"/></div></div></header><nav class="navbar navbar-light navbar-expand-lg mainmenu border rounded-lg"><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav" style="width:100%"><li class="col p-0 text-center"><a href="/">Home</a></li><li class="col p-0 text-center dropdown"><a class="dropdown-toggle" aria-expanded="false">News</a><ul class="dropdown-menu"><li class="col p-0 text-center"><a href="/news">News</a></li><li class="col p-0 text-center"><a href="/recruitment">Recruitment</a></li></ul></li><li class="col p-0 text-center dropdown"><a class="dropdown-toggle" aria-expanded="false">Events</a><ul class="dropdown-menu"><li class="col p-0 text-center"><a href="/event?type=calendar">Calendar</a></li><li class="col p-0 text-center"><a href="/event?type=seminars&amp;page=1">Seminars</a></li><li class="col p-0 text-center"><a href="/event?type=conference">Conferences &amp; Workshops</a></li><li class="col p-0 text-center"><a href="https://sites.google.com/view/reinact2021/home">ReInAct</a></li></ul></li><li class="col p-0 text-center dropdown"><a class="dropdown-toggle" aria-expanded="false">Projects</a><ul class="dropdown-menu"><li class="col p-0 text-center"><a href="http://www.christinehowes.com/research/drips">Dialogical Reasoning in Patients with Schizophrenia (DRiPS)</a></li><li class="col p-0 text-center"><a href="http://www.christinehowes.com/research/incred">Incremental Reasoning in Dialogue (IncReD)</a></li><li class="col p-0 text-center"><a href="https://wasp-hs.org/projects/gothenburg-research-initiative-for-politically-emergent-systems-gripes/">Gothenburg research initiative for politically emergent systems (GRIPES)</a></li></ul></li><li class="col p-0 text-center"><a href="/phd-courses">Courses</a></li><li class="col p-0 text-center dropdown"><a class="dropdown-toggle" aria-expanded="false">Research Groups</a><ul class="dropdown-menu"><li class="col p-0 text-center dropdown"><a class="dropdown-toggle" aria-expanded="false">Language and Perception</a><ul class="dropdown-menu"><li class="col p-0 text-center"><a href="/research/language-and-perception-group/">About</a></li><li class="col p-0 text-center"><a href="/research/language-and-perception-group/meetings/">Reading group</a></li><li class="col p-0 text-center dropdown"><a class="dropdown-toggle" aria-expanded="false">Courses</a><ul class="dropdown-menu"><li class="col p-0 text-center"><a href="/research/language-and-perception-group/courses/rom/rom/">Representations of Meaning</a></li><li class="col p-0 text-center"><a href="/research/language-and-perception-group/courses/apl/apl/">Language, Action, and Perception</a></li><li class="col p-0 text-center"><a href="/research/language-and-perception-group/courses/csoc/csoc/">Sociolinguistics and Bilingualism for NLP</a></li><li class="col p-0 text-center"><a href="/research/language-and-perception-group/courses/ml-vl/ml-vl/">ML Methods for Vision and Language</a></li></ul></li></ul></li><li class="col p-0 text-center dropdown"><a class="dropdown-toggle" aria-expanded="false">Dialogue</a><ul class="dropdown-menu"><li class="col p-0 text-center"><a href="/research/dialogue-group/">About</a></li><li class="col p-0 text-center"><a href="/research/dialogue-group/meetings/">Reading group</a></li></ul></li><li class="col p-0 text-center"><a href="/research/machine-learning-group/">Machine Learning</a></li><li class="col p-0 text-center"><a href="/research/type-theory-group/">Type Theory</a></li></ul></li><li class="col p-0 text-center"><a href="/people">People</a></li><li class="col p-0 text-center"><a href="/contact">Contact</a></li></ul></div></nav></div></div><div class="row"><div class="container"><div style="padding-top:20px;padding-bottom:20px" class="row"><div class="col"><div class="blog-post-container"><div class="blog-post"><h1></h1><h2></h2><div class="blog-post-content"><p>The situated language and perception reading group meets on even Fridays 10-12 in the seminar room on the 5th floor of Humanistiska fakulteten, Renströmsgatan 6. </p>
<p>Sometimes, and more recently we meet <a href="https://gu-se.zoom.us/j/726750116">online on Zoom</a>, requires GU-login.</p>
<p>From here you can also:</p>
<ul>
<li>add <a href="https://github.com/GU-CLASP/language-and-perception/blob/master/meetings.md">your paper suggestions</a></li>
<li>add <a href="https://linux.dobnik.net/cloud/index.php/s/Z2iPGa28Mexyz24">a paper</a></li>
<li>when adding a paper, please use a link to the published (e.g. ACL) version rather than arXiV if the former exists</li>
</ul>
<h3>Next</h3>
<ul>
<li>V&#x26;L paper</li>
</ul>
<h3>Suggestions</h3>
<p>Please add here any papers (with links) you would like to suggest for the reading group.</p>
<ul>
<li>M. Artetxe, G. Labaka, and E. Agirre. <a href="https://www.aclweb.org/anthology/P17-1042/">Learning bilingual word embeddings with (almost) no bilingual data.</a> In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 451–462, Vancouver, Canada, July 2017. Association for Computational Linguistics. (comes with a video and slides)</li>
<li>M. Artetxe, G. Labaka, and E. Agirre. <a href="https://www.aclweb.org/anthology/P18-1073">A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings.</a> In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 789–798, Melbourne, Australia, July 2018. Association for Computational Linguistics. (comes with a video and slides)</li>
<li>Sellam, T., Das, D., &#x26; Parikh, A. P. (2020). BLEURT: Learning Robust Metrics for Text Generation. <a href="https://arxiv.org/abs/2004.04696">https://arxiv.org/abs/2004.04696</a> (recommended by Nikolai)</li>
<li>Tan, H., &#x26; Bansal, M. (2019). LXMERT: Learning Cross-Modality Encoder Representations from Transformers. <a href="https://arxiv.org/abs/1908.07490">https://arxiv.org/abs/1908.07490</a> (recommended by Simon)</li>
<li>Wu, J., &#x26; Mooney, R. J. (2019). Self-Critical Reasoning for Robust Visual Question Answering. <a href="http://arxiv.org/abs/1905.09998">http://arxiv.org/abs/1905.09998</a> (recommended by Simon)</li>
<li>Joyce Y. Chai, Rui Fang, Changsong Liu, and Lanbo She. 2017. Collaborative language grounding to- ward situated human-robot dialogue. AI Magazine, 37(4):32–45.</li>
<li>Joyce Y. Chai, Qiaozi Gao, Lanbo She, Shaohua Yang, Sari Saba-Sadiya, and Guangyue Xu. 2018. Lan- guage to action: Towards interactive task learning with physical agents. In Proceedings of the Twenty- Seventh International Joint Conference on Artificial Intelligence (IJCAI-18).</li>
<li>Akbik, Alan &#x26; Blythe, Duncan &#x26; Vollgraf, Roland. (2018). Contextual String Embeddings for Sequence Labeling.
<a href="https://www.aclweb.org/anthology/C18-1139/">paper</a> (recommended by Axel)</li>
<li>Nguyen, Phi &#x26; Joty, Shafiq &#x26; Hoi, Steven &#x26; Socher, Richard. (2020). Tree-structured Attention with Hierarchical Accumulation. <a href="https://openreview.net/forum?id=HJxK5pEYvr">paper</a> (recommended by Axel)</li>
<li>Wang, Bin &#x26; Chen, Fenxiao &#x26; Wang, Yuncheng &#x26; Kuo, C.. (2020). Efficient Sentence Embedding via Semantic Subspace Analysis. <a href="https://arxiv.org/abs/2002.09620">paper</a> (recommended by Axel)</li>
<li>Goodman, N. D., &#x26; Stuhlmüller, A. (2013). Knowledge and Implicature: Modeling Language Understanding as Social Cognition. Topics in Cognitive Science. <a href="https://web.stanford.edu/~ngoodman/papers/GS-TopiCS-2013.pdf">paper</a> (recommended by Bill)</li>
<li>Tan, H., Dernoncourt, F., Lin, Z., Bui, T., &#x26; Bansal, M. (2019). Expressing Visual Relationships via Language. <a href="https://www.aclweb.org/anthology/P19-1182.pdf">paper</a> (recommended by Nikolai)</li>
<li>Thomason, J., Padmakumar, A., Sinapov, J., Walker, N., Jiang, Y., Yedidsion, H., ... &#x26; Mooney, R. J. (2020). Jointly improving parsing and perception for natural language commands through human-robot dialog. Journal of Artificial Intelligence Research, 67, 1-48. <a href="https://jair.org/index.php/jair/article/view/11485/26562">paper</a> (recommended by Mehdi)</li>
<li>Moro, D., Black, S., &#x26; Kennington, C. (2019). Composing and Embedding the Words-as-Classifiers Model of Grounded Semantics. arXiv preprint arXiv:1911.03283. (<a href="https://arxiv.org/pdf/1911.03283.pdf">https://arxiv.org/pdf/1911.03283.pdf</a>) (recommended by Staffan)</li>
<li>Mollica, F. et al. (2019). Composition is the core driver of the language-selective network <a href="https://www.biorxiv.org/content/10.1101/436204v2">Paper</a> (recommended by Mehdi)</li>
<li>Malt, B. C., Sloman, S. A., Gennari, S., Shi, M., &#x26; Wang, Y. (1999). Knowing versus naming: Similarity and the linguistic categorization of artifacts. Journal of Memory and Language, 40(2), 230-262. <a href="https://1b7a2906-a-62cb3a1a-s-sites.googlegroups.com/site/slomanlab/malt_et_al_jml.pdf?attachauth=ANoY7co8b_1l8eshBxSMrD65NRkjgq0cFvJZ-9XwdEVW9r2gOx-a8rOnyZPPV19Ngnx8x4hD2M7huNSJ8NlKz1XV6Zok8U9QutTZgyfoz4AB_k9hVNMjbmk3Skd1vFCOXTiW4f_PuvL3nQxEgVud3jdUqP_9nYWmNuL0aiMYohD9Kq8DiVFd1ywotrzNsTYzM0k5hygIkNyADjIXTGxZnaT80bobVBNp5Q%3D%3D&#x26;attredirects=0">paper</a> (recommended by Staffan)</li>
<li>Herbelot, A., &#x26; Vecchi, E. M. (2015). Building a shared world: Mapping distributional to model-theoretic semantic spaces. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (pp. 22-32).(<a href="https://www.cl.cam.ac.uk/~ah433/emnlp2015.pdf">https://www.cl.cam.ac.uk/~ah433/emnlp2015.pdf</a>) (recommended by Staffan)</li>
<li>Marcus, G. (2018). Deep learning: A critical appraisal. <a href="https://arxiv.org/pdf/1801.00631.pdf">paper</a>; <a href="https://www.youtube.com/watch?v=wh_IZNHH2S0">video comments</a>; (recommended by Mehdi)</li>
<li>J. A. Bateman, M. Pomarlan, and G. Kazhoyan. Embodied contextualization: Towards a multistratal ontological treatment. Applied Ontology, Pre-press:1–35, 2 October 2019. <a href="https://content.iospress.com/articles/applied-ontology/ao190218">paper</a></li>
<li>J. Thomason, M. Murray, M. Cakmak, and L. Zettlemoyer. Vision-and-dialog navigation. In Conference on Robot Learning (CoRL), 2019. <a href="https://arxiv.org/abs/1907.04957">paper</a> (recommended by Simon)</li>
<li><a href="https://youtu.be/P4wI938mx00">What are the differences between neural networks and the brain?</a> panel discussion from
Center for Brains, Minds and Machines (CBMM) (recommended by Mehdi)</li>
<li>W. N. Havard, J.-P. Chevrot, and L. Besacier. Models of visually grounded speech signal pay attention to nouns: a bilingual experiment on english and japanese. arXic, arXiv:1902.03052 [cs.CL]:1–5, 2019. <a href="https://arxiv.org/abs/1902.03052">paper</a> (recommended by Sylvie)</li>
<li>L. Arras, F. Horn, G. Montavon, K.-R. Müller, and W. Samek. ”What is relevant in a text document?”: An interpretable machine learning approach. PLOS ONE, 12(8):1–23, 08 2017. <a href="https://doi.org/10.1371/journal.pone.0181142">paper</a> (recommended by Felix)</li>
<li>M. Janner, K. Narasimhan, and R. Barzilay. Representation learning for grounded spatial reasoning. Transactions of the Association for Computational Linguistics, 6:49–61, 2018. (recommended by Mehdi) <a href="https://www.transacl.org/ojs/index.php/tacl/article/view/1234">link</a></li>
<li>Yatskar, M., Zettlemoyer, L., &#x26; Farhadi, A. (2016). Situation recognition: Visual semantic role labeling for image understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 5534-5542). <a href="https://homes.cs.washington.edu/~my89/publications/situations.pdf">link</a> (recommended by Mehdi)</li>
<li>Mei, H., Bansal, M., &#x26; Walter, M. R. (2016, February). Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences. In AAAI (pp. 2772-2778). <a href="https://arxiv.org/abs/1506.04089">link</a> (recommended by Mehdi)</li>
<li>J. Zwarts and Y. Winter. Vector space semantics: A model-theoretic analysis of locative prepositions. Journal of Logic, Language and Information, 9:169–211, 2000. (recommended by all)</li>
<li>one of the papers on this page (Oxford robotics &#x26; vision group): <a href="http://www.robots.ox.ac.uk/~nsid/publications.html">link</a> (recommended by Staffan)</li>
<li>Ben-Yosef, G., Assif, L., &#x26; Ullman, S. (2018). Full interpretation of minimal images. Cognition, 171, 65-84.
<a href="https://perso.telecom-paristech.fr/bloch/AIC/articles/BenYosef2017.pdf">link</a>
<a href="https://www.youtube.com/watch?v=2TWS_d843ys">video</a>
(recommended by Mehdi)</li>
</ul>
<h3>Read</h3>
<ul>
<li>Parizi, A. H., &#x26; Cook, P. (2020). <a href="https://www.aclweb.org/anthology/2020.lrec-1.330.pdf">Evaluating Sub-word embeddings in cross-lingual models.</a> Proceedings ofthe 12th Conference on Language Resources and Evaluation (LREC 2020), May, 2712–2719. (recommended by Tewodros) 2020-06-26</li>
<li>Pezzelle, S., &#x26; Fernández, R. (2019). Is the Red Square Big? MALeViC: Modeling Adjectives Leveraging Visual Contexts. arXiv preprint arXiv:1908.10285. <a href="https://arxiv.org/pdf/1908.10285.pdf">paper</a> (recommended by Staffan) 2020-06-12</li>
<li>Talk: Míriam Sánchez-Alcón: The significance of applying attention to Visual Question Answering  <a href="https://gubox.app.box.com/s/djn8w0k2qlmkgbdsr8yk0dsz22r1fjsj">paper</a> and  Wu, J., &#x26; Mooney, R. J. (2018). Faithful Multimodal Explanation for Visual Question Answering [cs.CL], 2020. <a href="http://arxiv.org/abs/1809.02805">paper</a> (recommended by Simon) 2020-05-29</li>
<li>Goodman, N. D., &#x26; Frank, M. C. (2016). Pragmatic Language Interpretation as Probabilistic Inference. In Trends in Cognitive Sciences. <a href="http://langcog.stanford.edu/papers_new/goodman-2016-underrev.pdf">paper</a> (recommended by Bill) 2020-05-15</li>
<li>Talk: David Alfter: Visual features in textual complexity classification: a case study on pictograms  <a href="https://gubox.box.com/shared/static/nuyn4p02bcj8pok1lmd9huf54wfmt8an.pdf">paper</a> and  Y. Bisk, A. Holtzman, J. Thomason, J. Andreas, Y. Bengio, J. Chai, M. Lapata, A. Lazaridou, J. May, A. Nisnevich, N. Pinto, and J. Turian. Experience grounds language. arXiv, arXiv:2004.10151 [cs.CL], 2020. <a href="https://arxiv.org/abs/2004.10151">paper</a> 2020-04-30</li>
<li>J. Krause, J. Johnson, R. Krishna, and L. Fei-Fei. A hierarchical approach for generating descriptive image paragraphs. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3337–3345, July 21–26 2017. <a href="https://arxiv.org/pdf/1611.06607.pdf">paper</a> (recommended by Nikolai)</li>
<li>Tai, Kai &#x26; Socher, Richard &#x26; Manning, Christoper. (2015). Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks. 1. 10.3115/v1/P15-1150. <a href="https://arxiv.org/abs/1503.00075">paper</a> (recommended by Axel) 2020-04-03</li>
<li>Cohn-Gordon, R., Goodman, N., &#x26; Potts, C. (2018). Pragmatically Informative Image Captioning with Character-Level Inference. <a href="http://arxiv.org/abs/1804.05417">paper</a> (recommended by Nikolai) 2020-03-20</li>
<li>Anonymous (2020), Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data.
<a href="https://openreview.net/forum?id=GKTvAcb12b">paper</a> (recommended by Mehdi) 2020-03-06 </li>
<li>X. Yu, H. Zhang, Y. Song, Y. Song, and C. Zhang. What you see is what you get: Visual pronoun coreference resolution in dialogues. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5122–5131, Hong Kong, China, Nov. 2019. Association for Computational Linguistics. <a href="https://www.aclweb.org/anthology/D19-1516">paper</a> (recommended by Sharid and Simon) 2019-12-13 </li>
<li>Research talk by <a href="http://insidr.nu/eton-ab/">Vaishnavi Annavarjula</a> 2019-12-02</li>
<li>F. Cavicchio, D. Melcher, and M. Poesio. The effect of linguistic and visual salience in visual world studies. Frontiers in Psychology, 5:176, 2014. <a href="https://www.frontiersin.org/article/10.3389/fpsyg.2014.00176">paper</a> (recommended by Sharid and Simon) 2019-11-15</li>
<li>S. Kottur, J. M. Moura, D. Parikh, D. Batra, and M. Rohrbach. Visual coreference resolution in visual dialog using neural module networks. In Proceedings of the European Conference on Computer Vision (ECCV), pages 153–169, 2018. (recommended by Sharid and Simon) <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Satwik_Kottur_Visual_Coreference_Resolution_ECCV_2018_paper.pdf">link</a> 2019-10-04</li>
<li>J. M. Cano Sant ́ın. Fast visual grounding in interaction: bringing few-shot learning with neural networks to an interactive robot. Masters in language technology (mlt), 30 hec, Department of Philosophy, Lin- guistics and Theory of Science (FLOV), University of Gothenburg, Gothenburg, Sweden, September 18 2019. Supervisor: Simon Dobnik and Mehdi Ghanimifard, examiner: Aarne Ranta. 2019-09-18</li>
<li>Peters, Matthew E., et al. "Dissecting contextual word embeddings: Architecture and representation." Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1499–1509 Brussels, Belgium, October 31 - November 4, 2018. <a href="https://aclweb.org/anthology/D18-1179">link</a> (recommended by Felix) 2019-05-03</li>
<li>Pragst, Louisa, et al. “On the Vector Representation of Utterances in Dialogue Context.” Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018), European Language Resource Association, 2018. <a href="http://aclweb.org/anthology/L18-1124">link</a> (recommended by Bill Nobel) 2019-04-05</li>
<li>Forestier S, Oudeyer P-Y.  (2017)  A Unified Model of Speech and Tool Use Early Development. Proceedings of the 39th Annual Meeting of the Cognitive Science Society. <a href="http://sforestier.com/node/32">link</a> (recommended by Sylvie Saget) 2019-03-08</li>
<li>Li, J., Chen, X., Hovy, E., &#x26; Jurafsky, D. (2016). Visualizing and Understanding Neural Models in NLP. In Proceedings of NAACL-HLT (pp. 681-691). <a href="http://www.aclweb.org/anthology/N16-1082">link</a> (recommended by Felix Morger) 2019-02-22</li>
<li>G. Collell, L. V. Gool, and M. Moens. Acquiring common sense spatial knowledge through implicit spatial templates. arXiv, arXiv:1711.06821 [cs.AI]:1–8, 2017. <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16232/16259">link</a> 2019-02-08</li>
<li>N. Schneider, J. D. Hwang, V. Srikumar, J. Prange, A. Blodgett, S. R. Moeller, A. Stern, A. Bitan, and O. Abend. Comprehensive supersense disambiguation of English prepositions and possessives. arXiv, arXiv:1805.04905 [cs.CL], 2018. (recommended by Bill) 2018-12-06</li>
<li>B. Landau and R. Jackendoff. “what” and “where” in spatial language and spatial cognition. Behavioral and Brain Sciences, 16(2):217–238, 255–265, 1993. Background: B. Landau. Update on “what” and “where” in spatial language: A new division of labor for spatial terms. Cognitive Science, 41(2):321–350, 2016. (recommended by Mehdi) 2018-11-22</li>
<li>A. Conneau, G. Kruszewski, G. Lample, L. Barrault, and M. Baroni. What you can cram into a single vector: Probing sentence embeddings for linguistic properties. arXiv, arXiv:1805.01070 [cs.CL], 2018. (recommended by Bill) <a href="https://arxiv.org/pdf/1805.01070.pdf">link</a> 2018-11-02 </li>
<li>W. Monroe, R. X. D. Hawkins, N. D. Goodman, and C. Potts. Colors in context: A pragmatic neural model for grounded language understanding. Transactions of the Association for Computational Linguistics, 5:325–338, 2017. 2018-10-15  (recommended by Simon and Mehdi) <a href="http://www.aclweb.org/anthology/Q17-1023">link</a></li>
<li>I. Vulić and N. Mrkšić. Specialising word vectors for lexical entailment. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1134–1145. Association for Computational Linguistics, 2018 (recommended by Bill) 2018-10-08 <a href="http://aclweb.org/anthology/N18-1103">link</a></li>
<li>ACL 2018 report by Mehdi Ghanimifard 2018-09-21: <a href="https://docs.google.com/presentation/d/13zAcf5jFV516Q-fL9gwS_0NahTc5_EWxLkap3sShc4E/edit?usp=sharing">link</a></li>
<li>Matteo Mossio and Dario Taraborelli. Action-dependent perceptual invariants: From ecological to sensorimotor approaches. Consciousness and cognition, 17(4):1324-1340, 2008. (recommended by Sylvie) 2018-06-01</li>
<li>S. C. Marsella and J. Gratch. Ema: A process model of appraisal dynamics. Cognitive Systems Research, 10(1):70–90, 2009. (recommended by Vlad) 2018-05-04</li>
<li>Viethen, Jette, Robert Dale, and Markus Guhe. "Generating subsequent reference in shared visual scenes: Computation vs. re-use." Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2011. <a href="https://aclanthology.info/pdf/D/D11/D11-1107.pdf">link</a> (recommended by Sylvie) 2018-03-23</li>
<li>Stefanie Tellex: Learning Models of Language, Action and Perception for Human-Robot Collaboration  <a href="https://www.youtube.com/watch?v=Yqn0kdS8dHE&#x26;t=3035s">video</a> 2018-03-16</li>
<li>J. Y. Chai, R. Fang, C. Liu, and L. She. Collaborative language grounding toward situated human-robot dialogue. AI Magazine, 37(4), 2016. (recommended by Mehdi and Simon) 2018-02-09</li>
<li>J. Pustejovsky. From affordances to events: Communicating action through language and gesture. Paper manuscript, Department of Computer Science, Brandeis University, Waltham, MA USA, January 2018. (recommended by Robin)</li>
<li>Fodor, J. (1998). There are no recognitional concepts; not even RED. Philosophical issues, 9, 1-14. (recommended by Staffan)  (2018-01-12)</li>
<li>J. Johnson, B. Hariharan, L. van der Maaten, J. Hoffman, F. Li, C. L. Zitnick, and R. B. Girshick. Inferring and executing programs for visual reasoning. CoRR, abs/1705.03633(n):n, 2017. <a href="https://arxiv.org/abs/1705.03633">link</a> (recommended by Mehdi) 2017-12-08</li>
<li>A. Lücking. Modeling co-verbal gesture perception in type theory with records. In Computer Science and Information Systems (FedCSIS), 2016 Federated Conference on, pages 383–392. IEEE, 2016. (recommended by Vlad)</li>
<li>M. Malinowski, M. Rohrbach, and M. Fritz. Ask your neurons: A neural-based approach to answering questions about images. In Proceedings of the IEEE International Conference on Computer Vision, pages 1–9, 2015. <a href="https://www.d2.mpi-inf.mpg.de/sites/default/files/iccv15-neural_qa.pdf">link</a> (recommended by Simon) 2017-10-27</li>
<li>H. M. Hersh and A. Caramazza. A fuzzy set approach to modifiers and vagueness in natural language. Journal of Experimental Psychology: General, 105(3):254, 1976. <a href="http://www.wjh.harvard.edu/~caram/PDFs/1976_Hersh_Caramazza_JEPG.pdf">link</a> (recommended by Staffan) 2017-10-13</li>
<li>J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Learning to compose neural networks for question answering. CoRR, abs/1601.01705:1–10, 2016. <a href="https://arxiv.org/abs/1601.01705">link</a> (recommended by Mehdi), 2017-09-29</li>
</ul>
<h3>Meetings</h3>
<h5>2017-02-14: Mehdi: Learning to compose spatial relations with grounded recurrent neural language models</h5>
<ul>
<li>Mehdi, Haris, Stegrios, Jean-Philippe, Chris, Robin, Yuri and Simon</li>
<li>Several interesting papers to read: <a href="https://docs.google.com/document/d/1MNzmOq0yAzmNaKSs5RJ6BcOiqEZdVLu5VzbCdg4SktQ/edit?usp=sharing">list of cited papers</a></li>
</ul>
<h5>2017-02-06: First, kick-off meeting</h5>
<ul>
<li>Mehdi, Haris, Chris, Robin and Simon</li>
<li>Areas of language and perception</li>
</ul></div></div></div></div></div></div></div><div class="row"><footer id="footer" style="width:100%"><div class="row"><div class="col-12 col-md-4 col"><div class="footer-header-type-style">Contact Information</div><p><b>Office Hours: </b>Monday-Friday (9.00am - 5.00pm)</p><p><b>Phone: </b>Phone +46 31-786 0000</p><p><b>E-mail: </b><a href="mailto:susanna.myyry@gu.se">susanna.myyry@gu.se</a></p></div><div class="col-12 col-md-4 col"><div class="footer-header-type-style">Quick Links</div><p><a href="/news">News</a></p><p><a href="https://gu-clasp.github.io/recruitment/" target="_blank">Recruitment</a></p><p><a href="https://clasp.gu.se" target="_blank">CLASP GU page</a></p><p><a href="https://www.gu.se/en" target="_blank">University of Gothenburg</a></p></div><div class="col-12 col-md-4 col"><div class="footer-header-type-style">Address</div><p>University of Gothenburg<br/>Department of Philosophy, Linguistics and Theory of Science <br/>CLASP - Centre for Linguistic Theory and Studies in Probability<br/>P.o. Box 200<br/>se-405 30 GOTHENBURG<br/>SWEDEN</p><a href="https://goo.gl/maps/kga7QHae7wfKfKNfA" target="_blank">Show Map</a></div></div></footer></div></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/research/language-and-perception-group/meetings/";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-d4801ee0a32a09229c1f.js"],"app":["/app-9846c3f7f4b3f0350ee6.js"],"component---src-pages-404-js":["/component---src-pages-404-js-71fbab4e444732050f7b.js"],"component---src-pages-contact-js":["/component---src-pages-contact-js-9db7986dbfafb9b9d453.js"],"component---src-pages-event-js":["/component---src-pages-event-js-715dc6f3143a95ac532b.js"],"component---src-pages-index-js":["/component---src-pages-index-js-5e902811d507af65f1cd.js"],"component---src-pages-news-js":["/component---src-pages-news-js-9229238085b13af41dc6.js"],"component---src-pages-people-js":["/component---src-pages-people-js-bc8259094e482874865c.js"],"component---src-pages-phd-courses-js":["/component---src-pages-phd-courses-js-81f7033338a2bd9cde60.js"],"component---src-pages-publication-js":["/component---src-pages-publication-js-a58ec3b16ee6b914a4c6.js"],"component---src-pages-recruitment-js":["/component---src-pages-recruitment-js-29c48941d44a87be9d00.js"],"component---src-pages-research-js":["/component---src-pages-research-js-1595c11c291aec7d21cf.js"],"component---src-templates-conference-template-js":["/component---src-templates-conference-template-js-d41a88b05dee04ed0951.js"],"component---src-templates-course-template-js":["/component---src-templates-course-template-js-d2fb3999c5e1aa7f7831.js"],"component---src-templates-markdown-template-js":["/component---src-templates-markdown-template-js-cf19b8e806f90d375072.js"],"component---src-templates-news-template-js":["/component---src-templates-news-template-js-bf42bbb2384d2e457e7f.js"],"component---src-templates-recruitments-template-js":["/component---src-templates-recruitments-template-js-48ab7d6105d2168d0192.js"],"component---src-templates-seminar-template-js":["/component---src-templates-seminar-template-js-82f16061dc7480995c1f.js"],"component---src-templates-staff-template-js":["/component---src-templates-staff-template-js-08c0ff82e19158435a87.js"],"component---src-templates-workshop-template-js":["/component---src-templates-workshop-template-js-9ba2d3565da19d5d813d.js"]};/*]]>*/</script><script src="/polyfill-d4801ee0a32a09229c1f.js" nomodule=""></script><script src="/component---src-templates-markdown-template-js-cf19b8e806f90d375072.js" async=""></script><script src="/commons-5dfeb980da566c46321b.js" async=""></script><script src="/app-9846c3f7f4b3f0350ee6.js" async=""></script><script src="/framework-073c72e9d2e4c2490fcd.js" async=""></script><script src="/webpack-runtime-df4e8c76addaafc8837b.js" async=""></script></body></html>